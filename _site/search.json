[
  {
    "objectID": "pages/technical_writing.html",
    "href": "pages/technical_writing.html",
    "title": "junwkim's blog",
    "section": "",
    "text": "LLM Agent\n\n\n대화형 Interaction에서 벗어나, Task를 수행하기 위한 LLM Agent를 소개합니다.\n\n\n\nJunwoo Kim\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/papers/DrQA/DrQA.html",
    "href": "pages/papers/DrQA/DrQA.html",
    "title": "DrQA: Reading Wikipedia to Answer Open-Domain Questions",
    "section": "",
    "text": "DrQA는 open-domain QA task에 대해, wikipedia와 같은 외부지식소스를 활용한 사실적인 답변을 하는 방식을 제안한다. 기계판독(machine reading) task는 연관성있는 document retrieval과 text에 대한 machine comprehension을 결합하는 과제이다. DrQA는 bi-gram hashing과 tf-idf matching으로 동작하는 방식과 multi-layer rnn모델의 훈련으로 Wikipedia에서 답변을 탐지해내는 두 검색의 요소를 결합하는 방식을 제안한다. 제안된 방법은 이미 존재하는 QA데이터셋에서 월등한 성능향상을 보였고, multi-task learning과 관련한 supervision 확보에서도 유효함을 증명했다."
  },
  {
    "objectID": "pages/papers/DrQA/DrQA.html#abstract",
    "href": "pages/papers/DrQA/DrQA.html#abstract",
    "title": "DrQA: Reading Wikipedia to Answer Open-Domain Questions",
    "section": "",
    "text": "DrQA는 open-domain QA task에 대해, wikipedia와 같은 외부지식소스를 활용한 사실적인 답변을 하는 방식을 제안한다. 기계판독(machine reading) task는 연관성있는 document retrieval과 text에 대한 machine comprehension을 결합하는 과제이다. DrQA는 bi-gram hashing과 tf-idf matching으로 동작하는 방식과 multi-layer rnn모델의 훈련으로 Wikipedia에서 답변을 탐지해내는 두 검색의 요소를 결합하는 방식을 제안한다. 제안된 방법은 이미 존재하는 QA데이터셋에서 월등한 성능향상을 보였고, multi-task learning과 관련한 supervision 확보에서도 유효함을 증명했다."
  },
  {
    "objectID": "pages/papers/DrQA/DrQA.html#introduction",
    "href": "pages/papers/DrQA/DrQA.html#introduction",
    "title": "DrQA: Reading Wikipedia to Answer Open-Domain Questions",
    "section": "Introduction",
    "text": "Introduction\nWikipedia를 odqa task에서의 데이터셋으로 사용하는 시도는 오래전부터 있어왔지만, 아쉽게도 기계가 아닌 human interested로 구축되어 있다. 그렇지만 Wikipedia는 수백~수천만 개의 document로 구성되어 있기 때문에 large scale opqa와 기계의 text comprehension에 활용되어 왔다.\n통상적으로 그 어떠한 질문에라도 답변하려면 두 단계를 거쳐야하는데, 먼저 500만이 넘는 answer 후보 중에 relevant articles를 retrieve해야하고, 그렇게 찾은 후보군 중에서 조심스럽게 답변을 식별해야한다(identify). 이러한 단계구축을 MRS(machine reading at scale) 라고 명명한다. 이러한 단계로 구성하면 internal graph등을 구축할 필요가 없다는 장점이 존재한다. 또한, 이러한 방식은 지식소스에 대한 일반화를 달성하여 어떤 종류의 문서뭉치, 책, 심지어 news와 같은 daily paper에도 적용될 수 있다.\nIBM이 제안한 DeepQA같은 large-scale QA 시스템은 Wikipedia가 아닌 매우 많은 answer source를 활용한다. 또한 이런 소스들은 이미 구축된 KnowledgeBase Pair를 사용한다. 결과적으로 이러한 답변은 소스간의 중복성에 영향을 받아 답변이 매우 정확해지고 뾰족해지는 현상을 유발한다. (아마도 검색용도로는 정확하되 진정한 의미의 machine reading이 아니 다 라는 말을 하고 싶었던 것 같음)\n때문에 이런 배경을 토대로 machine reading ability에 대한 연구와 SQuAD나 CNN/Daily Mail, CBT와 같은 데이터셋의 생성으로 이어졌다.\n그러나, 이러한 machine comprehension 접근들은 연관있는 문서에 대한 short piece text를 미리 구축하고 모델에 조건으로 준다는 것을 가정하고 있고, 이것은 진정한 의미의 open- domain question answering에 부합하지 않는다. 이러한 접근들은 검색솔루션의 일부로 취급되어야 한다. MRS에 촛점을 맞추면 기계이해를 한다는 맥락은 유지하면서도 하위문서에 대한 이해와 현실적인 large open resource를 유지할 수 있다.\n논문에서는 여러 종류의 QA 데이터셋에 대해 MRS를 평가할때 제안된 시스템이 강력하게 동작하는 것을 증명할 것이다. Wikipedia를 활용한 Strong QA 시스템(DrQA)는 다음과 같은 요소들로 구성된다.\n\nDocument Retriever: bigram hash / TF-IDF matching으로 수백만개의 문서에서 효율적으로 연관문서 subset를 찾아주는 module\nDocument Reader: 수집된 연관문서를 입력으로 하는 multi-layer rnn네트워크로써, 답변의 범위를 탐지하고 답변하도록 훈련된 module\n\n 이러한 방식으로 훈련된 DrQA의 Document Retriever는 위키피디아의 내장 검색엔진보다 더 강력하고 ,아주 많은 데이터셋(SQuAD 포함)에서 SOTA를 달성하는 성능을 보여준다. 특히 단일 task traning보다 multi-task training에서 더 강력함을 보여준다."
  },
  {
    "objectID": "pages/papers/DrQA/DrQA.html#related-work",
    "href": "pages/papers/DrQA/DrQA.html#related-work",
    "title": "DrQA: Reading Wikipedia to Answer Open-Domain Questions",
    "section": "Related Work",
    "text": "Related Work\nOpen-Domain QA는 원래 구조화되지 않은 수집된 문서들에 대해 답변된 TREC competition으로부터 유래했다. KnowledgeBased의 발전에 따라 QA를 잘하려는 많은 시도들이 있어왔지만, 대부분 KBs의 근본적인 한계(incompleteness, fixed schemas)를 극복하지 못해 결국은 raw text로부터 곧바로 answering을 수행하는 방향으로 선회되었다.\nmachine comprehension에 관한 두번째 motivation은 기계가 짧은 text 혹은 story를 듣고 난 후에 답변하는 것이다. 이러한 접근방법은 attention based 그리고 memory augmented neural network 등의 딥러닝 아키텍쳐 연구와 함께 진행되었다. 이런 방향의 연구들은 open-domain QA task에서 좋은 성능과 새로운 framework들을 제안해왔다.\n한편, Wikipedia를 활용한 연구에서는 대부분 구조화된 Knowledge Base Model이 주를 이루었는데, 이러한 방식들은 미리 문서로부터 접근가능한 sub-text 등을 뽑고 카테고리와 같은 여러가지 pattern들을 미리 준비시켜놓는 방식이었다.\nDrQA는 오로지 주어진 text에 대해서만 고려하며, Wikipedia text documents는 오로지 MRS에 보조되는 용도로만 고려한다.\n그 외에도 AskMSR(MS), DeepQA(IBM) 등 QA Pipeline을 구축하는 관점에서의 연구들은 대부분 KnowledgeBase를 활용한 검색시스템 구축 관점에서의 연구성과이지 온전하게 text comprehension을 바탕으로 한 QA시스템으로 보기는 어려웠다.\n\nDocument Retriever\nClassical QA system을 따라 machine learning을 사용하지 않는 document retrieval system을 사용한다. document retriever는 어느정도 유의미한 articles을 찾아서 돌려준다. 보통 inverted index를 바라보고 각 term에 대한 vector scoring model을 통해 점수를 계산하여 질문에 대한 답변을 찾는다. 이러한 작업은 Wikipedia 내장 Search API인 Elastic- search에서도 잘 동작한다. Qustion과 Articles는 TF-IDF weighted bag-of-words vectors로 비교된다.\n또한 여기서 더 나아가 local word에 대한 n-gram features를 통해 더 나은 성능을 보였다. Feature hashing for large scale multitask learning 논문에서 제안하는 hashing 방법과 bigram을 통해 메모리 효율적이고 속도도 향상된 시스템을 개발했다.\nDrQA에서 개발된 document retriever는 5개의 wikipedia articles를 반환하도록 설정되었다. 이렇게 검색된 document는 document reader에 의해 처리된다.\n\n\nDocument Reader\nDocument Retriever는 machine comprehension task에서 큰 성공을 거든 AttentiveReader의 구조를 차용한 neural network로 구성된다. document retreiver의 정의는 다음과 같다.\n\n주어진 quesetion q를 구성하는 \\(l\\) tokens를 \\(\\{q_1, ..., q_l\\}\\)로 한다.\nsmall set of documents의 \\(n\\) paragraphs 중에서 하나를 선택한 paragraphs \\(p\\)를 구성하는 \\(m\\) tokens를 \\(\\{p_1, ..., p_m\\}\\)으로 한다.\n\nRNN Model은 각 paragraph를 입력으로 받아 나온 출력을 aggregate하여 answer를 predict하는 task를 수행한다. 좀 더 구체화하여 나눠서 정의하면 다음과 같다.\n\nParagraph Encoding\n모든 토큰 p_i를 d차원의 sequence of feature vector인 \\(p^{~}_{i}\\)로 RNN을 통과시켜 생성한다.\n\\({p_1, ... , p_m} = RNN({p˜1, . . . , p˜m})\\)\nparagraph bold pi는 token pi로부터 유의미한 정보들을 가진 context information으로 압축된다. 이때, multi-layer bidirectional lstm을 사용하여 각 layer들의 end hidden units의 출력값을 취합한다. 이렇게 생성된 feature vector p~i는 다음과 같이 구성된다.\n\n\nWord Embeddings\n300차원의 Glove word embedding을 사용한다. 대부분의 word embedding은 고정하고, 1000여개 정도의 most frequent question words에 대해서만 fine-tune what, how, which, many 등의 단어는 QA System에서는 매우 치명적이고 중요하기 때문\n\n\nExact Match\n\\(f_{exact\\_match}(pi) = I(pi ∈ q).\\)\n3개의 simple한 binary features를 사용했는데, pi(토큰)이 질문에 들어있는 단어인지, lower-case문자인지, 혹은 lemma form(유사한뜻)인지 여부에 따라 표기한다. 이러한 feature의 사용은 매우 강력하고 Section 5에서 살펴볼것.\n\n\nToken Features\n\\(f_{token}(pi) = (POS(pi), NER(pi), TF(pi)).\\)\ntoken p{i}의 일부 속성을 반영하는 몇가지 feature를 추가한다. 품사(Part Of Speeach, POS), NER(Name Entity Recognition), 용어빈도 TF(Term Frequency) 등이다.\n\n\nAligned Question Embedding\nDrQA 작성당시의 논문추세로, question embedding을 할당한다.\n\\(f_{align}(pi) = \\Sigma_{j} a_{i,j} E(q_{j})\\) 로 표현되며, attention score \\(a_{i,j}\\)는 paragraph token pi와 each question words \\(q_j\\)의 similarity로 동작한다.\n\\(a_{i,j}\\) score는 dot-products로 계산되어 비선형적인 word embedding으로 구성된다.\n\n\n\nAligned Question Embedding\n\n\n위 식에서 알파는 ReLU를 포함한 single dense layer이다. 이런식으로 학습하면 비슷하지만 Non-identical words를 학습하는데에 도움이 된다.(car & vehicle)\n\n\n\nPrediction\nParagraph Level에서, 우리의 목표는 가장 정답과 근사한 span of tokens를 예측하는 것이다. 우리는 Paragraph vectors {p1, . . . , pm}과 question vector q를 입력으로 가지고 있고, 두개의 classifier를 통해 두개의 ends of the span을 예측하도록 독립적으로 학습한다."
  },
  {
    "objectID": "pages/introduce.html",
    "href": "pages/introduce.html",
    "title": "junwkim's blog",
    "section": "",
    "text": "안녕하세요. 제 이름은 김준우입니다. 서울/경기권에 거주하고 있고, 현재는 Machine Learning Engineer로 Gmarket에서 근무하고 있습니다. AI/ML 알고리즘과 엔지니어링 요소를 활용하여 세상에 유용한 application을 제공하는데에 관심이 있습니다."
  },
  {
    "objectID": "pages/introduce.html#소개글",
    "href": "pages/introduce.html#소개글",
    "title": "junwkim's blog",
    "section": "",
    "text": "안녕하세요. 제 이름은 김준우입니다. 서울/경기권에 거주하고 있고, 현재는 Machine Learning Engineer로 Gmarket에서 근무하고 있습니다. AI/ML 알고리즘과 엔지니어링 요소를 활용하여 세상에 유용한 application을 제공하는데에 관심이 있습니다."
  },
  {
    "objectID": "pages/introduce.html#career",
    "href": "pages/introduce.html#career",
    "title": "junwkim's blog",
    "section": "Career",
    "text": "Career\n\nMakinarocks(Machine Learning Engineer)\n\n2020.01 - 2021.07 (Seoul)\n태양광 발전량 예측 프로젝트(SK 계열사 협업프로젝트)\n\n일기예보 데이터 분석\nRegression 기반 모델개발(Tree Based, NN Based)\n\n발전량 예측서비스 개발\n\nAWS EKS 기반 SaaS 개발\n\n\n\n\nMusinsa(Search Engineer)\n\n2021.08 - 2022.07 (Seoul)\n이미지 검색서비스 개발\n\n기 사용 Solution을 Internal로 전환하여 비용절감\nObject Detection, Classification, Vector Embedding 모델 개발\nElasticsearch 기반 vector 검색 및 Airflow Pipeline 개발\nAWS EKS를 활용한 Microservice 개발\n\n연관검색어 추천기능 개발\n\n검색결과가 없는 페이지(SNR)에서의 UX 지표개선\nFast-Text 기반 vector embedidng 모델개발\nContinuous Training / Update 가능한 Airflow Pipeline 개발\n연관검색결과를 제공하는 Internal API Server 개발\n\n팀내 Python Convention 및 Data 분석 Lead\n\n\n\nGmarket(Machine Learning Engineer)\n\n2022.08 - Present\n홈개인화 서비스개발\n\n비개인화 유저대상 GMV 26% 개선\nPhase 1(tag 검색기반 개인화)\n\nlong/short term 유저행동을 기반으로 Tag 추출\nElasticsearch에 적재된 상품풀에 검색하여 ranking되는 상품리스트 제공\nElasticsearch Query Tunning 및 Spring API Server 개발\n\nPhase 2(vector 검색기반 semantic 개인화)\n\n전문검색(Full-Text) 기반 개인화의 상품 편중현상 해결\nNext Step Item을 예측하는 Transformer 기반의 vector model 개발\nElasticsearch 기반 vector 검색 및 Airflow Pipeline 개발\nCPU 추론 최적화를 통한 GPU 자원절약\n\nPhase 3(re-ranking을 통한 micro 개인화)\n\n개인화된 ranking 제공 이후, User Action에 따른 Optimized Ranking 제공 및 Filter Bubble 해결\nItem Score / User Score 집계 후, 합산계산하는 방식의 Re-Ranking list 제공"
  },
  {
    "objectID": "pages/posts/nlp_interview/nlp_interview.html",
    "href": "pages/posts/nlp_interview/nlp_interview.html",
    "title": "NLP Interview List",
    "section": "",
    "text": "토큰화는 주어진 텍스트를 토큰(Token)이라 불리는 작은 단위로 나누는 과정\n“I love natural language processing!”이라는 문장을 토큰화하면 [“I”, “love”, “natural”, “language”, “processing”, “!”]\n단순히 공백을 기준으로 나누는 것부터 형태소 분석, 서브워드 토큰화(Subword Tokenization) 등 다양한 기법\nSubword Tokenization\n\n단어를 더 작은 단위인 서브워드(Subword)로 분리하는 토큰화(birthplace = birth + place)\n기존 단어 수준 토큰화에서는 학습 데이터에 등장하지 않은 단어(OOV)를 처리\n단어 수준 토큰화는 어휘 크기가 매우 크고 희소한 단어들로 인해 행렬의 차원이 높아지는 문제\n대표적으로 가장 Byte Pair Encoding(BPE)가 있음\n\n\n\n\nBPE Encoding\n\n\n\n\n\n\n\n\n\n단어나 토큰을 고정된 크기의 실수 벡터로 표현하는 방법\n전통적으로는 one-hot encoding을 사용했지만 의미적 거리를 측정하지 못하고, 차원의 저주가 발생\n전 학습된 워드 임베딩(dense vector)로 표현하는 것이 일반적\nWord2Vec\n\ngoogle이 개발한 워드임베딩 알고리즘\nCBOW(Continuous Bag of Words), Skip-Gram 두가지 방식으로 학습을 제공\nCBOW\n\n\n\n\nSkip-Gram\n\n\n\n\n\nFast-Text\n\nGoogle이 개발한 Sub-word 기반의 워드임베딩 알고리즘\n각 단어를 n-gram으로 분할하여 sub-word로 다루기 때문에 OOV에도 대응가능하고 학습속도가 빠르며 단어의 형태학적 특성을 학습가능\napple에 대한 n-gram tokenize\n\nn=3) &lt;ap, app, ppl, ple, le&gt;, \nn=6) &lt;ap, app, ppl, ppl, le&gt;, &lt;app, appl, pple, ple&gt;, &lt;appl, pple&gt;, …, \n\n\n\n\n\n\n\n주어진 단어 시퀀스의 확률 분포를 추정하는 작업\n\n“The cat sits on the”라는 문장이 주어졌을 때, 언어 모델은 다음 단어로 “mat”, “couch”, “floor” 등이 올 확률을 계산\n\n\\(P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2|w_1) * P(w_3|w_1, w_2) * ... * P(w_n|w_1, w_2, ..., w_{n-1})\\)\nNLP에서 사용되는 평가 지표(Metrics)\n\nPerplexity\n\n언어 모델이 실제 텍스트 데이터를 얼마나 잘 생성하는지를 측정 모델이 할당한 확률의 역수로 계산되며, 값이 낮을수록 좋은 모델\n\n문장에 대한 Generation probability의 역수의 기하평균\n혹은 \\(PPL(W) = \\sqrt[N]{\\frac{1}{P(w_1, w_2, w_3, ...., w_N)}} = \\sqrt[N]{\\frac{1}{\\displaystyle \\prod_{i=1}^{N} P(w_i | w_1, w_2, w_3, ...., w_{i-1})}}\\)\n\n\nBLEU(bilingual evaluation understudy)\n\n번역기가 번역한 문장이 사람이 정한 정답 문장과 유사할 수록 더 높은 스코어\nGenerated Sentence의 단어가 Reference Sentence에 포함되는 정도로 측정\n\n\\({w_{gen}∈S_{ref}|w_{gen}∈S{gen}}/|S_{gen}|\\)\n\n\nROUGE\n\nReference Setence의 단어가 Generated Sentence에 포함되는 정도\n{w{ref}∈S{gen}|w{ref}∈S{ref}}/|S{ref}|"
  },
  {
    "objectID": "pages/posts/nlp_interview/nlp_interview.html#자연어-처리-nlp-기초",
    "href": "pages/posts/nlp_interview/nlp_interview.html#자연어-처리-nlp-기초",
    "title": "NLP Interview List",
    "section": "",
    "text": "토큰화는 주어진 텍스트를 토큰(Token)이라 불리는 작은 단위로 나누는 과정\n“I love natural language processing!”이라는 문장을 토큰화하면 [“I”, “love”, “natural”, “language”, “processing”, “!”]\n단순히 공백을 기준으로 나누는 것부터 형태소 분석, 서브워드 토큰화(Subword Tokenization) 등 다양한 기법\nSubword Tokenization\n\n단어를 더 작은 단위인 서브워드(Subword)로 분리하는 토큰화(birthplace = birth + place)\n기존 단어 수준 토큰화에서는 학습 데이터에 등장하지 않은 단어(OOV)를 처리\n단어 수준 토큰화는 어휘 크기가 매우 크고 희소한 단어들로 인해 행렬의 차원이 높아지는 문제\n대표적으로 가장 Byte Pair Encoding(BPE)가 있음\n\n\n\n\nBPE Encoding\n\n\n\n\n\n\n\n\n\n단어나 토큰을 고정된 크기의 실수 벡터로 표현하는 방법\n전통적으로는 one-hot encoding을 사용했지만 의미적 거리를 측정하지 못하고, 차원의 저주가 발생\n전 학습된 워드 임베딩(dense vector)로 표현하는 것이 일반적\nWord2Vec\n\ngoogle이 개발한 워드임베딩 알고리즘\nCBOW(Continuous Bag of Words), Skip-Gram 두가지 방식으로 학습을 제공\nCBOW\n\n\n\n\nSkip-Gram\n\n\n\n\n\nFast-Text\n\nGoogle이 개발한 Sub-word 기반의 워드임베딩 알고리즘\n각 단어를 n-gram으로 분할하여 sub-word로 다루기 때문에 OOV에도 대응가능하고 학습속도가 빠르며 단어의 형태학적 특성을 학습가능\napple에 대한 n-gram tokenize\n\nn=3) &lt;ap, app, ppl, ple, le&gt;, \nn=6) &lt;ap, app, ppl, ppl, le&gt;, &lt;app, appl, pple, ple&gt;, &lt;appl, pple&gt;, …, \n\n\n\n\n\n\n\n주어진 단어 시퀀스의 확률 분포를 추정하는 작업\n\n“The cat sits on the”라는 문장이 주어졌을 때, 언어 모델은 다음 단어로 “mat”, “couch”, “floor” 등이 올 확률을 계산\n\n\\(P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2|w_1) * P(w_3|w_1, w_2) * ... * P(w_n|w_1, w_2, ..., w_{n-1})\\)\nNLP에서 사용되는 평가 지표(Metrics)\n\nPerplexity\n\n언어 모델이 실제 텍스트 데이터를 얼마나 잘 생성하는지를 측정 모델이 할당한 확률의 역수로 계산되며, 값이 낮을수록 좋은 모델\n\n문장에 대한 Generation probability의 역수의 기하평균\n혹은 \\(PPL(W) = \\sqrt[N]{\\frac{1}{P(w_1, w_2, w_3, ...., w_N)}} = \\sqrt[N]{\\frac{1}{\\displaystyle \\prod_{i=1}^{N} P(w_i | w_1, w_2, w_3, ...., w_{i-1})}}\\)\n\n\nBLEU(bilingual evaluation understudy)\n\n번역기가 번역한 문장이 사람이 정한 정답 문장과 유사할 수록 더 높은 스코어\nGenerated Sentence의 단어가 Reference Sentence에 포함되는 정도로 측정\n\n\\({w_{gen}∈S_{ref}|w_{gen}∈S{gen}}/|S_{gen}|\\)\n\n\nROUGE\n\nReference Setence의 단어가 Generated Sentence에 포함되는 정도\n{w{ref}∈S{gen}|w{ref}∈S{ref}}/|S{ref}|"
  },
  {
    "objectID": "pages/posts/nlp_interview/nlp_interview.html#transformers",
    "href": "pages/posts/nlp_interview/nlp_interview.html#transformers",
    "title": "NLP Interview List",
    "section": "Transformers",
    "text": "Transformers\n\nNote | GPT-2,3 등으로부터 최신 GPT 서비스에 이르기까지 LLM의 가장 기본이 되는 이론적 토대들은 RNN으로 부터 파생\n\n\nRNN\n\nCommon NeuralNetwork은 Input Size가 고정되어 있어 입력길이가 가변적인 자연어 Task를 풀기에는 부적합\nRNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징\nRNN Architecture\n\n\n\nSequence To Sequence\n\nRNN으로 이미 one-to-many, many-to-many task를 풀 수 있었지만, RNN으로 encoder-decoder 구조를 구성할 수 있음\n입력 문장과 출력 문장의 길이가 다를 경우에 사용하는데, 대표적인 분야가 번역기나 텍스트 요약과 같은 경우\n\n\n인코더 RNN 셀의 마지막 시점의 은닉 상태를 디코더 RNN 셀로 넘겨주는데 이를 컨텍스트 벡터\n컨텍스트 벡터는 디코더 RNN 셀의 첫번째 은닉 상태\ndecoder에서의 입력순서로 인해 &lt;sos&gt; 토큰과 &lt;eos&gt; 토큰을 관리해줘야함\n\n\n\n\n\nAttention\n\nRNN에 기반한 seq2seq 모델에는 크게 두 가지 문제\n\n하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생\nRNN의 고질적인 문제인 기울기 소실(vanishing gradient) 문제\n\nKey Idea of Attention\n\n디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 것\n모든 입력문장을 전부 참조하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)\n\n\\(Attention(Q, K, V) = Attention Value\\)\n\n\n어텐션 함수는 주어진 ’쿼리(Query)’에 대해서 모든 ’키(Key)’와의 유사도를 각각 구함\n구해낸 이 유사도를 키와 맵핑되어있는 각각의 ’값(Value)’에 반영\n유사도가 반영된 ’값(Value)’을 모두 더해서 리턴(Attention Value 라고 부름)\nsequence to sequence 모델을 기준으로 다음과 같이 정의\n\nQ = Query : t 시점의 디코더 셀에서의 은닉 상태\nK = Keys : 모든 시점의 인코더 셀의 은닉 상태들\nV = Values : 모든 시점의 인코더 셀의 은닉 상태들\n\n\nDot-Product Attention\n\n(To Be Continue)"
  },
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "junwkim's blog",
    "section": "",
    "text": "technical writing\n\n\n  LLM Agent Junwoo Kim, May 10, 2024 \n\n\nNo matching items\n\n\n\nPapers\n\n\n  RAG: Retrieval Augmented Generation for Knowledge-Intensive NLP Task. Junwoo Kim, Dec 27, 2023 \n  DrQA: Reading Wikipedia to Answer Open-Domain Questions Junwoo Kim, Dec 24, 2023 \n\n\nNo matching items\n\n\n\nPosts\n\n\n  NLP Interview List Junwoo Kim, Apr 14, 2024 \n  GPTs Service로 개인블로그 만들기 Junwoo Kim, Mar 4, 2024 \n\n\nNo matching items"
  },
  {
    "objectID": "pages/posts/gpt_base_blog/gpt_base_blog.html",
    "href": "pages/posts/gpt_base_blog/gpt_base_blog.html",
    "title": "GPTs Service로 개인블로그 만들기",
    "section": "",
    "text": "오랫동안 notion, confluence wiki 등 많은 블로그 서비스를 시도해 왔습니다. 어떤 도구는 지나치게 자유도가 높고 어떤 도구는 자유도가 낮은 대신 많은 기능을 제공했지만, 오히려 그게 불편해져서 그 중간 사이 어딘가의 포스팅 도구를 찾았습니다.\n마침 같은 팀에서 근무하는 분이 Quarto를 추천해주셨고, 이를 활용해 블로그를 개발할 때 Claude3(Opus, Sonnet)을 적극적으로 사용했는데 아마도 직접적으로 프론트업계에 종사하지 않는 엔지니어/과학자 분들에게 도움이 될까 싶어 그 과정을 남깁니다."
  },
  {
    "objectID": "pages/posts/gpt_base_blog/gpt_base_blog.html#개요",
    "href": "pages/posts/gpt_base_blog/gpt_base_blog.html#개요",
    "title": "GPTs Service로 개인블로그 만들기",
    "section": "",
    "text": "오랫동안 notion, confluence wiki 등 많은 블로그 서비스를 시도해 왔습니다. 어떤 도구는 지나치게 자유도가 높고 어떤 도구는 자유도가 낮은 대신 많은 기능을 제공했지만, 오히려 그게 불편해져서 그 중간 사이 어딘가의 포스팅 도구를 찾았습니다.\n마침 같은 팀에서 근무하는 분이 Quarto를 추천해주셨고, 이를 활용해 블로그를 개발할 때 Claude3(Opus, Sonnet)을 적극적으로 사용했는데 아마도 직접적으로 프론트업계에 종사하지 않는 엔지니어/과학자 분들에게 도움이 될까 싶어 그 과정을 남깁니다."
  },
  {
    "objectID": "pages/posts/gpt_base_blog/gpt_base_blog.html#quarto-소개",
    "href": "pages/posts/gpt_base_blog/gpt_base_blog.html#quarto-소개",
    "title": "GPTs Service로 개인블로그 만들기",
    "section": "Quarto 소개",
    "text": "Quarto 소개\nQuarto 데이터 사이언스 홈페이지에서는\n“Quarto 는 Pandoc에 기반한 오픈소스 과학기술 출판시스템이다. 하지만 특정 언어에 종속되지 않고 R, 파이썬, 쥴리아, 자바스크립트(Observable JS) 를 지원하고 있으며 이를 통해 다음 출판 저작물 작성이 가능하다.”\n라고 설명하고 있습니다. 개인적으로는 깔끔한 Layout을 제공하고, 마크다운 문법을 기반으로 동작하며 필요시에는 어렵지 않게 커스터마이징 할 수 있다는게 가장 큰 장점으로 생각합니다.\n\nInstall\nQuarto 홈페이지에 들어가서 Get Started 탭을 누르면 자신의 환경에 따라 설치할 수 있는 가이드를 제시해줍니다. 구글링으로 검색하면 여러가지 terminal command를 제시해주지만 해당 문서에서 가이드하는대로 시도하는게 가장 좋습니다. 저는 MacOS 유저이고, 터미널에서 vanilla vim으로 작업을 좋아하기 때문에 Quarto를 설치하기만 했습니다.\n\nhttps://quarto.org/docs/get-started/\n\n\n\n첫 화면\nQuarto를 사용하면 가장 먼저 _quarto.yaml을 작성해야합니다. 사실 글 제목인 Claude에게 물었을 때는 main.qmd를 먼저 작성하라고 안내해줬지만, 지금 작업하는 것이 웹페이지로 배포하기 위함이라는 사실을 인지시켜주면 다음과 같이 답변해줍니다.\n\n\n\nhow to build quarto website blog?\n\n\n실제로는 저는 다음과 같은 _quarto.yml을 작성했습니다.\nproject:\n  type: website\n  preview:\n    port: 4200\n\nwebsite:\n  title: \"junwkim's blog\"\n  description: \"technical writing about CS/CE\"\n  navbar:\n    background: \"#0a3e3c\"\n    foreground: White\n    left:\n      - href: pages/introduce.qmd\n        text: introduce\n    right:\n      - href: main.qmd\n        text: Back To Home\n\nformat:\n  html:\n    css: styles.css\n    js: script.js\n위 yaml 파일에서 명시하는 것은 다음과 같습니다.\n\n페이지의 이름은 “junkim’s blog” 일 것\nNavigation Bar 설정\n\nNavigation Bar의 색상은 Dark Teal(#0a3e3c) 일것\nintroduce page를 왼쪽, Back To Home page를 오른쪽에 둘 것.\n\n\n한편, _quarto.yml 기반으로 동작하는 quarto application은 index.qmd 를 가장 먼저 보도록 설정되어 있습니다. index.qmd 파일은 다음과 같이 작성해주었습니다.\n---\npagetitle: \"Home\"\n---\n\n```{=html}\n&lt;meta http-equiv=\"refresh\" content=\"0; url='./main.html'\" /&gt;\nindex.qmd에서는 홈의 가장 기본으로 바라보는 페이지가 main.qmd에서 그려주는 형상이 되도록 지정했습니다. 따라서 블로그의 어디에서든 홈에 접속한다면 main.qmd가 그려주는 페이지로 진입하게 됩니다.\n\n\n\nhome main page\n\n\n\n\n세부 화면 구성\n보시면 아시겠지만, 지금 보는 화면은 저희가 작성한 것보다는 좀더 많은 내용들이 추가가 되어 있습니다. 먼저 introduce라는 페이지를 구성하겠습니다.\n---\ntitle: \"\"\nformat:\n  html:\n    toc: false\n---\n\n## 소개글\n안녕하세요. 제 이름은 김준우입니다.\n서울/경기권에 거주하고 있고, 현재는 Machine Learning Engineer로 Gmarket에서 근무하고 있습니다. AI/ML 알고리즘과 엔지니어링 요소를 활용하여 세상에 유용한 application을 제공하는데에 관심이 있습니다.\n이렇게 작성한 마크다운 문법은 다음과 같이 나타납니다. \n이어서, 홈화면에 나타난 카드 횡스크롤 방식의 UI를 구현해보겠습니다. 요즘 넷플릭스처럼 콘텐츠를 가로로 스크롤하며 제공해주는 방식의 UI가 매우 유행하고 있습니다. 제가 생각하기로는 콘텐츠가 아주 많아진 요즘 시대에는 거의 필수에 가까울 것으로 생각되는 디자인 요소 중 하나라고 생각합니다.\nQuarto에서는 내가 작성한 게시글들이라는 콘텐츠를 Listing해서 보여주는 기능이 있습니다. 홈페이지에서 제공하는 document에서 다양한 형식의 quarto content 작성방식을 제공하고 있습니다.\n\nhttps://quarto.org/docs/reference/projects/websites.html#project\n\n위 링크에서 Listing 항목을 살펴보면 어떤 콘텐츠를 담을 것인지, List의 이름은 무엇으로 할 것인지, 최대로 보여줄 콘텐츠의 갯수, 정렬방식을 지정할 수 있음을 알 수 있습니다. 특히, type 항목에서 List 혹은 Grid를 지정할 수 있습니다. 물론 List와 Grid 또한 충분히 이쁜 UI지만 저희가 하고 싶은 것은 횡스크롤 형식의 카드UI입니다.\nlisting:\n    - id: \"list1\"\n      contents: pages/posts\n      sort: \"date asc\"\n      type: custom\n      categories: false\n      sort-ui: false\n      filter-ui: false\n      feed: false\n      template: listing.ejs\n저는 실제로는 문서정독을 통해 EJS(Embedded JavaScript)라는 기능을 통해 사용자가 직접 정의한 UI를 입힐 수 있음을 파악했지만, 갖은 노력을 다해도 도저히 구현할 수 없었습니다. 그래서 Claude에게 다시한번 물어보았습니다. Claude는 EJS 문법 작성과 CSS파일 생성을 거의 대부분 수행하였지만, 사소한 오류와 에러들이 있었고 이를 바로 잡는데에 대부분의 시간을 보냈습니다.\n\n\n\nclaude도 디버깅을 잘 못할 수 있다.\n\n\n현존 GPTs 서비스 중 가장 강력하다고 알려진 Claude 또한 이미지 입력까지 제공받았음에도 유료 API Quoto를 3번이나 소진할 떄까지 올바른 방법을 제시하지 못했습니다. 그러던 중 클로드가 다음과 같은 제안을 해왔습니다.\n\nstyle.css\n\n.card-container {\n  display: flex;\n  overflow-x: auto;\n  scroll-snap-type: x mandatory;\n  scroll-padding: 1rem;\n}\n\n.card {\n  flex: 0 0 200px;\n  margin-right: 1rem;\n  padding: 1rem;\n  background: #f1f1f1;\n  border-radius: 5px;\n  scroll-snap-align: center;\n  text-decoration: none;\n  color: inherit;\n  display: flex;\n  flex-direction: column;\n}\n\n.card h3 {\n  margin: 0;\n}\n\nlisting.ejs\n\n&lt;div class=\"card-container\"&gt;\n&lt;% for (const post of collections.all.pages/posts) { %&gt;\n  &lt;a href=\"&lt;%= post.url %&gt;\" class=\"card\"&gt;\n    &lt;h3&gt;&lt;%= post.data.title %&gt;&lt;/h3&gt;\n  &lt;/a&gt;\n&lt;% } %&gt;\n&lt;/div&gt;\n위 코드를 적용하자, 드디어 초기에 제가 원하던 컨셉과 비슷한 화면 view가 생성되었습니다! \n한번 기능이 완성되고 나서부터는 원하는 UI를 구현하는 것은 비교적 빠르게 수행 가능했습니다. 사소한 디버깅이나 레이아웃을 수정하는 오류는 claude를 통해 해결하였고, 추가적인 style 변화를 위해 몇가지 요소를 추가했습니다.\n.card-container {\n  display: flex;\n  overflow-x: auto;\n  scroll-snap-type: x mandatory;\n  scroll-padding: 1rem;\n}\n\n.card {\n  width: 200px;\n  height: 265px;\n  margin-right: 1rem;\n  padding: 1rem;\n  background: #e0f2ef;\n  border-radius: 5px;\n  scroll-snap-align: center;\n  text-decoration: none;\n  color: inherit;\n  display: flex;\n  flex-direction: column;\n  box-sizing: border-box;\n  position: relative;\n}\n\n.card-text {\n  display: -webkit-box;\n  -webkit-line-clamp: 3;\n  -webkit-box-orient: vertical;\n  overflow: hidden;\n  text-overflow: ellipsis;\n  word-break: break-word;\n  line-height: 1.2;\n  max-height: calc(1.2em * 3);\n  margin: 0.5rem;\n}\n\n.card-image {\n  width: 100%;\n  height: 160px;\n  marin: 1rem;\n  border-top-left-radius: 5px;\n  border-top-right-radius: 5px;\n  border: 1px solid #e0e0e0;\n}\n적용한 카드 스타일은 다음과 같습니다.\n\ncard container를 통해 횡 scroll UI를 구현\ncard style 정의를 통해 가로/세로 높이와 카드 배경색 조정, 테두리 둥근 효과 등을 부여\ncard-text style 정의를 통해 카드 내부의 text는 어떤 높이에서 시작하고 어디까지 표시될지, 표시영역을 벗어나면 어떻게 처리될지를 정의\ncard-image style 정의를 통해 카드 내부에 이미지가 표기되도록 하고, 해당 이미지의 여러 특성을 정의\n\n\n\n정리하며\n이러한 작업을 하면서 놀라운 부분은, 저는 프론트에 대해서는 아주 기본적인 내용들만 알고 있었을 뿐임에도 Claude와 함께 상호작용하며 저만의 멋진 블로그를 만드는데에 그렇게 큰 힘이 들지 않았다는 것입니다. 뿐만 아니라, 제가 작성한 대부분의 코드는 Claude가 생성해 준 것이고 제가 고려한 부분은 디자인적 요소 외에는 거의 없었다고 봐도 무방할 것 같습니다.\n만약 이 글을 읽으시는 독자분께서 블로그는 갖고 싶지만 프론트를 잘 모르신다고 한다면, Claude를 기반으로 아주 맨땅에서부터 헤딩해도 좋은 결과물을 얻을 수 있을 것이라 생각합니다. 끝으로, 여태까지 작업한 코드는 다음 github에서 참고하실 수 있습니다.\n\nhttps://github.com/junwoo-ctrl/junwkim-blog-quarto"
  },
  {
    "objectID": "pages/papers/RAG/RAG.html",
    "href": "pages/papers/RAG/RAG.html",
    "title": "RAG: Retrieval Augmented Generation for Knowledge-Intensive NLP Task.",
    "section": "",
    "text": "일반적으로 LLM은 방대한 양의 데이터를 기반으로 사전학습(Pre-Trained)된 GPT Based Model을 의미한다. GPT는 Next-Token-Prediction을 수행하는 Pretraining 과정을 거치며 자연어 테스크들에서 유의미한 성과들을 보여왔다.\nLLM을 활용하려는 시도 중 하나는 LLM에 질문을 하고 올바른(품질이 좋은) 답변을 얻는 것이다. 이러한 분야를 Open-Domain Question Answering(ODQA)라고 정의한다. 따라서 ODQA에서는 input text를 question으로, output text를 answer로 하는 Framework로 정의한다.\n\n\n\n\n\ngraph LR\nA[Who was Cleopatra?] --&gt; B[LLM]\nB --&gt; C[Cleopatra was a famous Egyptian queen.]\n\nstyle A fill:#fff,stroke:#000,color:#000\nstyle B fill:#fff,stroke:#000,color:#000\nstyle C fill:#fff,stroke:#000,color:#000\nlinkStyle 0 stroke:#000, stroke-width:1px, stroke-linecap:butt;\nlinkStyle 1 stroke:#000, stroke-width:1px, stroke-linecap:square;\n\n\n\n\n\n\n질문을 입력하면 답변을 생성한다는 점에서 LLM을 QA Task에서 생성하는 것이 굉장히 주목을 받게 되었다. 그런데 답변을 생성해내는 것까지는 좋은 일인데, LLM에 아주 잘못된 입력을 해도 그럴듯하게 답변을 생성해내는 Hallucination 현상이 대두되었다.\n\n\n\n\n\ngraph LR\nA[Who is the brave Korean Cleopatra?]\nstyle A fill:#fff,stroke:#000,color:#000\n\nB[LLM]\nstyle B fill:#fff,stroke:#000,color:#000\n\nC[She was a very brave and famous independence activist.]\nstyle C fill:#fff,stroke:#000,color:#000\n\nA --&gt; B\nlinkStyle 0 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\nB --&gt; C\nlinkStyle 1 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\n\n\n\n\n\nExample of Hallucination\n또한, QA Task의 용도로 사용하기에는 단편적이고 간단한 답변을 내놓는 경향이 있다. 이러한 단점을 보강하기 위해 한동안은 Hallucination을 피하고 답변의 생성을 도와주기 위해 Context를 붙이는 방식이 선호되었다.\n\n\n\n\n\ngraph LR\nA[Who is the brave Korean Cleopatra?]\nstyle A fill:#fff,stroke:#000,color:#000\n\nB[LLM]\nstyle B fill:#fff,stroke:#000,color:#000\n\nC[She was a very brave and famous independence activist.]\nstyle C fill:#fff,stroke:#000,color:#000\n\nsubgraph Context\nD[We are talking about ancient romain history.]\nstyle D fill:#fff,stroke:#000,color:#000\nend\nstyle Context fill:#fff,stroke:#000,color:#000\n\n\nA --&gt; B\nlinkStyle 0 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\nB --&gt; C\nlinkStyle 1 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\nD --&gt; B\nlinkStyle 2 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\n\n\n\n\n\nContext를 입력 프롬프트에 추가하는 방식이 동작하는 이유는 근본적으로 GPT는 Next Token Prediction을 수행하는 모델이기 때문이다. 앞서 등장한 Token들과 연관이 큰 답변을 생성해야하니 Context를 잘 조절하면 Hallucination을 막는것은 물론 질 좋은 답변을 생성하는 데에도 도움이 된다.\n그러나 세상에 존재하는 Question은 사실상 무한하고, Question마다 의미있는 Context를 생성하는 작업은 무척 비효율적이다. 또한, Knowledge Intensive한 경우 Context를 생성하는 비용 자체가 비싸다는 단점이 있다. RAG(Retrieval Augmented Generation)은 이러한 문제를 해결하기 위해 제시되었다."
  },
  {
    "objectID": "pages/papers/RAG/RAG.html#large-language-model-open-domain-question-answering",
    "href": "pages/papers/RAG/RAG.html#large-language-model-open-domain-question-answering",
    "title": "RAG: Retrieval Augmented Generation for Knowledge-Intensive NLP Task.",
    "section": "",
    "text": "일반적으로 LLM은 방대한 양의 데이터를 기반으로 사전학습(Pre-Trained)된 GPT Based Model을 의미한다. GPT는 Next-Token-Prediction을 수행하는 Pretraining 과정을 거치며 자연어 테스크들에서 유의미한 성과들을 보여왔다.\nLLM을 활용하려는 시도 중 하나는 LLM에 질문을 하고 올바른(품질이 좋은) 답변을 얻는 것이다. 이러한 분야를 Open-Domain Question Answering(ODQA)라고 정의한다. 따라서 ODQA에서는 input text를 question으로, output text를 answer로 하는 Framework로 정의한다.\n\n\n\n\n\ngraph LR\nA[Who was Cleopatra?] --&gt; B[LLM]\nB --&gt; C[Cleopatra was a famous Egyptian queen.]\n\nstyle A fill:#fff,stroke:#000,color:#000\nstyle B fill:#fff,stroke:#000,color:#000\nstyle C fill:#fff,stroke:#000,color:#000\nlinkStyle 0 stroke:#000, stroke-width:1px, stroke-linecap:butt;\nlinkStyle 1 stroke:#000, stroke-width:1px, stroke-linecap:square;\n\n\n\n\n\n\n질문을 입력하면 답변을 생성한다는 점에서 LLM을 QA Task에서 생성하는 것이 굉장히 주목을 받게 되었다. 그런데 답변을 생성해내는 것까지는 좋은 일인데, LLM에 아주 잘못된 입력을 해도 그럴듯하게 답변을 생성해내는 Hallucination 현상이 대두되었다.\n\n\n\n\n\ngraph LR\nA[Who is the brave Korean Cleopatra?]\nstyle A fill:#fff,stroke:#000,color:#000\n\nB[LLM]\nstyle B fill:#fff,stroke:#000,color:#000\n\nC[She was a very brave and famous independence activist.]\nstyle C fill:#fff,stroke:#000,color:#000\n\nA --&gt; B\nlinkStyle 0 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\nB --&gt; C\nlinkStyle 1 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\n\n\n\n\n\nExample of Hallucination\n또한, QA Task의 용도로 사용하기에는 단편적이고 간단한 답변을 내놓는 경향이 있다. 이러한 단점을 보강하기 위해 한동안은 Hallucination을 피하고 답변의 생성을 도와주기 위해 Context를 붙이는 방식이 선호되었다.\n\n\n\n\n\ngraph LR\nA[Who is the brave Korean Cleopatra?]\nstyle A fill:#fff,stroke:#000,color:#000\n\nB[LLM]\nstyle B fill:#fff,stroke:#000,color:#000\n\nC[She was a very brave and famous independence activist.]\nstyle C fill:#fff,stroke:#000,color:#000\n\nsubgraph Context\nD[We are talking about ancient romain history.]\nstyle D fill:#fff,stroke:#000,color:#000\nend\nstyle Context fill:#fff,stroke:#000,color:#000\n\n\nA --&gt; B\nlinkStyle 0 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\nB --&gt; C\nlinkStyle 1 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\nD --&gt; B\nlinkStyle 2 stroke:#000, stroke-width:1px, stroke-linecap:butt;\n\n\n\n\n\n\nContext를 입력 프롬프트에 추가하는 방식이 동작하는 이유는 근본적으로 GPT는 Next Token Prediction을 수행하는 모델이기 때문이다. 앞서 등장한 Token들과 연관이 큰 답변을 생성해야하니 Context를 잘 조절하면 Hallucination을 막는것은 물론 질 좋은 답변을 생성하는 데에도 도움이 된다.\n그러나 세상에 존재하는 Question은 사실상 무한하고, Question마다 의미있는 Context를 생성하는 작업은 무척 비효율적이다. 또한, Knowledge Intensive한 경우 Context를 생성하는 비용 자체가 비싸다는 단점이 있다. RAG(Retrieval Augmented Generation)은 이러한 문제를 해결하기 위해 제시되었다."
  },
  {
    "objectID": "pages/papers/RAG/RAG.html#abstract",
    "href": "pages/papers/RAG/RAG.html#abstract",
    "title": "RAG: Retrieval Augmented Generation for Knowledge-Intensive NLP Task.",
    "section": "Abstract",
    "text": "Abstract\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures.\n\n대규모 사전훈련 모델은 사실적인 정보들을 파라미터에 저장하고, 이를 통해 downstream NLP task에서 state-of-the-art 결과를 보여왔다.\n그런 연구들에서 지식을 정밀하게 조작하고 접근하는 능력은 한계가 있었고, 그러므로 이러한 구조는 knowledge-intensive한 task들에서는 성능저하가 발생했다.\n\nAdditionally, providing provenance for their decisions and updating their world knowledge remain open research problems.\n\n추가적으로, 모델의 decisions에 대한 근거를 제공하거나 새로운 knowledge를 업데이트 하는 것은 여전히 문제로 남아있다\n\nWe explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\n우리는 검색증강 생성(RAG)을 위한 fine-tunning recipe를 찾아보았고, 언어 생성을 위한 pretrained parametric 모델과 non-parametric memory로 구성된 모델구조를 제안한다.\nRAG 모델은 사전훈련된 Seq2Seq 모델과 Dense Vector로 구성된 Knowledge Index, 그리고 Index에 접근하기 위한 pre-trained neural retriever로 구성되었다."
  },
  {
    "objectID": "pages/papers/RAG/RAG.html#introduction",
    "href": "pages/papers/RAG/RAG.html#introduction",
    "title": "RAG: Retrieval Augmented Generation for Knowledge-Intensive NLP Task.",
    "section": "Introduction",
    "text": "Introduction\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data. They can do so without any access to an external memory, as a parameterized implicit knowledge base. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations”.\n\npre-trained neural language models은 데이터로부터 상당한 양의 지식을 배우는 것으로 보여졌고, external memory나 knowledge base에 접근하지 않고서도 가능하다고 여겨졌다.\n이런 발전은 흥미로웠지만, memory를 쉽게 확장하거나 수정하지 못했고 prediction에 대한 직접적인 근거를 제시하지 못했고, Hallucinations를 발생시켰다.\n\nHere, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n\n우리는 NLP Task에서는 정석이라고 할 수 있는 Seq2Seq 모델을 활용한 Parametric Memory와 Non-Parametric Memory를 구성했다.\nRAG models의 parametric memory는 seq2seq transformer이고, non-parametric memory는 wikipedia를 활용한 dense vector index이다. 이때의 dense vector를 생성하는 pre-trained neural retriever를 포함한다.\n\n\n\n\nRAG Architecture"
  },
  {
    "objectID": "pages/papers/RAG/RAG.html#method",
    "href": "pages/papers/RAG/RAG.html#method",
    "title": "RAG: Retrieval Augmented Generation for Knowledge-Intensive NLP Task.",
    "section": "Method",
    "text": "Method\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y.\n\n우리가 RAG model을 다룰 때, input sequence x에 대해 text document 집합인 z를 검색하고, 이러한 z들을 additional context로 사용하여 target sequence y를 생성하는데에 사용한다.\n\nIn one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.\n\ngeneration 단계에서 z를 사용하는 방식에 따라 RAG-Sequence와 RAG-Token 방식으로 구분한다.\nRag-Sequence은 각 token(1개의 output prediction에 사용된)을 예측하기 위해 같은 문서를 사용하고, Rag-Token은 각 token을 예측하기 위해 다른 문서를 사용한다\n\n\nModels\n\nRetriever: DPR \n\\(p_{\\eta}(z|x) \\propto\\ exp \\left(d(z)\\right)^T q(x)\\)\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE.\n\nRAG의 retriever는 DPR로부터 가져온 것이다.\nDPR은 Bert로 구성된 Bi-Encoder 구조를 가지고 있는데, \\(d(z)\\)는 document를 색인할 벡터 데이터베이스를 생성하는 용도의 encoder이고, \\(q(x)\\)는 입력 쿼리를 encoding하는 용도이다.\nNon-Parametric Memory라고 한다.\n\n\n\nRAG-Sequence\nThe RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,\n\\(P_{RAG-Sequence}(y|x) \\approx \\sum_{z \\in top-k(p(\\cdot|x))} p_{\\eta}(z|x)p_{\\theta}(y|x,z) = \\sum_{z \\in top-k(p(\\cdot|x))} p_{\\eta}(z|x) \\prod_{i}^{N} p_{\\theta}(y_{i}|x,z,y_{1:i-1})\\)\n\n1개의 output prediction이 token \\(\\{y_1, ..., y_N\\}\\)로 구성되었다고 가정한다.\n\\(y_i\\)번째 token을 구할 때, \\(y_1\\)부터 \\(y_{i-1}\\)까지의 token과 latent document z1을 사용한다.\n\n\\(p(y_1) \\leftarrow p(x, z_1, y_0)\\) , 아직 \\(y_0\\)는 존재하지 않음\n\\(p(y_2) \\leftarrow p(x, z_1, y_1)\\)\n\\(p(y_3) \\leftarrow p(x, z_1, y_{(1, 2)})\\)\n\\(p(y_4) \\leftarrow p(x, z_1, y_{(1, 2, 3)})\\)\n…\n\\(p(y_N) \\leftarrow p(x, z_1, y_{1:N-1})\\)\n\nk개의 z를 retriever로부터 찾아왔다는 가정이므로 k개의 document에 대해 반복할 수 있다.\n각 자리(\\(1, .., N\\))별로 존재하는 K개의 Token을 marginalize한다.\n\n\n\nRAG-Token\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\n\\(P_{RAG-TOKEN}(y|x) \\approx \\prod_{i}^{N} \\sum_{z \\in top-k(p(\\cdot|x))}p_{\\eta}(z|x)p_{\\theta}(y_{i}|x,z,y_{1:i-1})\\)\n\n1개의 output prediction이 token \\(\\{y_1, ..., y_N\\}\\)로 구성되었다고 가정한다.\n\\(y_i\\)번째 token을 구할 때, \\(y_1\\)부터 \\(y_{i-1}\\)까지의 token을 준비한다.\n\n이 토큰들과 latent \\(z_1\\)를 가지고 \\(y_i\\) token 확률을 구한다.\n이 토큰들과 latent \\(z_2\\)를 가지고 \\(y_i\\) token 확률을 구한다.\n…\n이 토큰들과 latent \\(z_k\\)를 가지고 \\(y_i\\) token 확률을 구한다.\n\n구해진 k개의 token 후보들을 가지고 marginalize하여 \\(y_i\\)번째 token을 구한다.\n\n\n\nGenerator: BART\nThe generator component pθ(yi |x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions.\n\ngenerator \\(p_{\\theta} (y_{i}|x, z, y_{1:i-1})\\) 는 BART-Large 400M 모델을 사용한 encoder-decoder 구조이다.\ninput query x와 retrieved document z를 심플하게 concatenate하였다.\nParametric Memory라고 부른다."
  },
  {
    "objectID": "pages/papers.html",
    "href": "pages/papers.html",
    "title": "junwkim's blog",
    "section": "",
    "text": "DrQA: Reading Wikipedia to Answer Open-Domain Questions\n\n\nODQA의 시작점이 되는 논문을 소개합니다.\n\n\n\nJunwoo Kim\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRAG: Retrieval Augmented Generation for Knowledge-Intensive NLP Task.\n\n\nLLM이 가진 약점을 타파하려는 현시점 가장 유의미한 시도를 정리합니다.\n\n\n\nJunwoo Kim\n\n\nDec 27, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/posts.html",
    "href": "pages/posts.html",
    "title": "junwkim's blog",
    "section": "",
    "text": "GPTs Service로 개인블로그 만들기\n\n\nClaude3만으로 Quarto 기반 블로그 만들기\n\n\n\nJunwoo Kim\n\n\nMar 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNLP Interview List\n\n\nNLP 면접질문을 대비합니다. GPT 기반으로 작성되었습니다.\n\n\n\nJunwoo Kim\n\n\nApr 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/technical_writing/llm_agent/progress.html",
    "href": "pages/technical_writing/llm_agent/progress.html",
    "title": "LLM Agent",
    "section": "",
    "text": "기본적으로 Large Language Model은 OpenAI, Google 등 Product API이든, Open-Source 모델이든 대화형 형식을 갖추고 채팅을 할 수 있는 기능만을 갖추고 있습니다. 또한, 이미 학습되어 있는 지식의 한도 안에서만 답변을 할 수 있습니다. 이는 우리가 기대하는 AGI 혹은 원하는 기대효과를 가지고 동작하게 할 수 있는 도구로써의 활용과는 차이가 있습니다.\nLLM Agent는 이러한 사용패턴에서 벗어나 자동화된 추론과 의사결정을 가능케하는 일종의 Engine이라고 생각할 수 있습니다. 우리가 원하는 어플리케이션은 기업 내 시스템에서의 데이터베이스에서 데이터를 가지고 와서 이를 기반으로 분석하여 답변을 내야할 수도 있고, 검색엔진을 통해 인터넷의 데이터를 가지고 와서 검색된 문서를 기반으로 답변을 만들 수 도 있어야합니다.\n이러한 역할을 수행하려면, LLM이 적절한 Tool을 선택해야하고 Tool을 사용하여 수행한 결과를 다시 사용자에게 전달할 수 있어야합니다. 이러한 일을 수행하는 개체가 바로 LLM Agent라고 할 수 있습니다."
  },
  {
    "objectID": "pages/technical_writing/llm_agent/progress.html#개요",
    "href": "pages/technical_writing/llm_agent/progress.html#개요",
    "title": "LLM Agent",
    "section": "",
    "text": "기본적으로 Large Language Model은 OpenAI, Google 등 Product API이든, Open-Source 모델이든 대화형 형식을 갖추고 채팅을 할 수 있는 기능만을 갖추고 있습니다. 또한, 이미 학습되어 있는 지식의 한도 안에서만 답변을 할 수 있습니다. 이는 우리가 기대하는 AGI 혹은 원하는 기대효과를 가지고 동작하게 할 수 있는 도구로써의 활용과는 차이가 있습니다.\nLLM Agent는 이러한 사용패턴에서 벗어나 자동화된 추론과 의사결정을 가능케하는 일종의 Engine이라고 생각할 수 있습니다. 우리가 원하는 어플리케이션은 기업 내 시스템에서의 데이터베이스에서 데이터를 가지고 와서 이를 기반으로 분석하여 답변을 내야할 수도 있고, 검색엔진을 통해 인터넷의 데이터를 가지고 와서 검색된 문서를 기반으로 답변을 만들 수 도 있어야합니다.\n이러한 역할을 수행하려면, LLM이 적절한 Tool을 선택해야하고 Tool을 사용하여 수행한 결과를 다시 사용자에게 전달할 수 있어야합니다. 이러한 일을 수행하는 개체가 바로 LLM Agent라고 할 수 있습니다."
  },
  {
    "objectID": "pages/technical_writing/llm_agent/progress.html#architecture",
    "href": "pages/technical_writing/llm_agent/progress.html#architecture",
    "title": "LLM Agent",
    "section": "Architecture",
    "text": "Architecture\n\n\n\nOverview of LLM-Agent"
  },
  {
    "objectID": "pages/technical_writing/llm_agent/progress.html#agent-system-overview",
    "href": "pages/technical_writing/llm_agent/progress.html#agent-system-overview",
    "title": "LLM Agent",
    "section": "Agent System Overview",
    "text": "Agent System Overview\n\n\n\nOverview of LLM-Agent\n\n\nLLM 기반의 자동화 Agent 시스템에서 LLM은 일종의 두뇌역할을 하고, 다음과 같은 components들로 구성됩니다.\n\nPlanning\n\nSub-goal / Decomposion\n\nAgent는 큰 작업을 더 작고 관리가능한 하위목표로 분해(decompose)하여 복잡한 작업을 처리할 수 있도록 합니다.\n\nReflection / Refinement\n\n과거 행동에 대해 자기비판과 성찰을 할 수 있고, next step을 위해 작업을 개선하여 final result의 결과를 향상시킵니다.\n\n\n\n\nMemory\n\nShort-Term Memory\n\nPrompt Engineering을 Short-Term Memory로 생각하여 유용한 학습(in-context learning)으로 활용한다고 볼 수 수 있습니다.\n\nLong-Term Memory\n\nLLM이 External Database(vector db 등), 확장된 검색결과를 활용하여 장기간의 혹은 무한한 “information”을 유지하고 회상할 수 수 있는 능력을 갖게 됩니다.\n\n\n\n\nTool Use\n\nPre-Training 이후에는 모델 가중치를 변경하기 어려운 상황에서 확보해야하는 추가 정보(최신 정보, Code 실행능력, source code에 대한 access 능력) 등을 사용하는 방법을 갖게 됩니다."
  },
  {
    "objectID": "pages/technical_writing/llm_agent/progress.html#planning-1",
    "href": "pages/technical_writing/llm_agent/progress.html#planning-1",
    "title": "LLM Agent",
    "section": "Planning",
    "text": "Planning\n일반적으로 User의 Query를 수행하기 위해서는 매우 많은 작업단계들을 거쳐야합니다. Agent가 유저의 지시를 파악하여 실행할 계획을 세우는 것에서부터 출발합니다.\n\nDecomposition\n\nChain of Thought\n\n복잡한 작업에 대한 모델성능을 향상시키는 표준 prompting 기법입니다. 주어진 지시문에 “Step by Step”으로 생각하라는 지시를 추가합니다.\n큰 작업을 여러 개의 관리가능한 작업으로 변환하고, 모델의 사고 과정에 대한 해석을 볼 수 있습니다.\n\nTree of Thoughts\n\nThought Decomposition\n\n문제를 여러 개의 “Thought” 단계로 분할합니다..\n각 생각은 문제 해결을 향한 중간 단계를 나타냅니다.\n생각의 단위는 몇 글자(Crosswords)부터 한 문단(Creative Writing)이 될 수 있습니다.\nLLM이 생성하고 평가할 수 있을만한 “충분히 작고 충분히 커야” 합니다.\n\nThought Generator\n\n현재 상태에서 가능한 다음 생각을 생성하는 개체입니다.\n현재 주어진 tree states에서 다음 생각단계에 대한 k개의 후보 Thought를 생성하기 위한 전략을 고려합니다.\nThought Range가 넓은 경우(Thought가 문단규모일 때)\n\nIdentically Independent Distribution 방식으로 각 생각을 Sampling\n\nThought Space가 한정적인 경우(Thought가 단어, 혹은 한줄)\n\n“Proposed Prompt”를 사용하여 순차적으로 생각을 제안합니다.\n\n\nState Evalutator\n\n현재 상태를 평가하여 문제 해결에 얼마나 가까운지 판단합니다.\n이는 독립적으로 각 상태를 평가(현재 상태를 숫자 값이나 클래스로 평가)하거나, 여러 상태를 비교(vote prompt를 사용하여 상태 간 비교 및 투표)하여 선택하는 방식으로 이루어질 수 있습니다.\n\n\n\n\n\nSelf-Reflection\nself-reflection은 agent가 과거의 행동을 성찰하고 이전의 실수를 수정하여 발전할 수 있게 하는 역할을 합니다. Agent가 실제 세계의 작업을 하다보면 피할 수 없는 시행착오를 개선하는 역할을 합니다.\n\nReAct\n\n모델에게 생각하고 행동하는 방식을 제공하는 기법입니다.\nThought-Action-Observation 과정을 거치며 문제를 해결합니다.\n\n\nReAct Process\n\n\n\n\n\n\n\n\nStep\nDetail\n\n\n\n\nQuestion\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\n\nAction 1\nSearch[Colorado orogeny]\n\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\n\nAction 2\nLookup[eastern sector]\n\n\nObservation 2\n(Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\n\nAction 3\nSearch[High Plains]\n\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\n\nThought 4\nI need to instead search High Plains (United States).\n\n\nAction 4\nSearch[High Plains (United States)\n\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\n\nAction 5\nFinish[1,800 to 7,000 ft]\n\n\n\n\n\nReflexion\n\n기존의 정책기반 강화학습과 달리, 언어적 피드백을 통해 agent를 강화하는 방식을 의미합니다.\nActor, Evaluator, Self-Reflection 3가지 요소로 구분됩니다.\n\nActor는 행동을 생성하고, Evaluator는 이를 평가하며, Self-Reflection 모델은 실패에 대한 반성을 생성합니다.\n\n의사결정(AlfWorld), 추론(HotPotQA), 코딩(HumanEval) 등 다양한 task에서 Reflexion이 아주 큰 성능 향상을 보였습니다.\n\n\n\nIllustration of the Relexion framework\n\n\n\nHeuristic 함수는 경로가 너무 비효율적이거나, 환각이 포함되어 있으면 중지할지 말지를 결정합니다.\n비효율적이다라는 것은 성공하지 못한채 너무 오래 반복하고 있는 것을 의미합니다.\n환각은 동일한 관찰결과를 초래하는 연속적인 동일 행동 시퀀스를 마주하는 것으로 정의됩니다.\n\n\nChain of Hindsight\n\n(작성예정)"
  },
  {
    "objectID": "pages/technical_writing/llm_agent/progress.html#memory-1",
    "href": "pages/technical_writing/llm_agent/progress.html#memory-1",
    "title": "LLM Agent",
    "section": "Memory",
    "text": "Memory\n\nTypes of Memory\n메모리는 정보를 획득, 저장, 보유, 혹은 나중에 retrieve하는데에 사용되는 프로세스로 정의할 수 있습니다. 인간의 기억은 다음과 같이 구분할 수 있습니다. \n\nSensory memory는 raw inputs, text, images 혹은 여타 modal들의 학습된 embedding 표현으로 볼 수 있습니다.\nShort-term memory는 in-context learning으로 볼 수 있습니다. transformers의 유한한 context window 길이처럼 제한되고, 짧고, 유한합니다.\nLong-term memory는 Agent가 쿼리시점에 주목할 수 있는 외부벡터 저장소로 볼 수 있습니다."
  },
  {
    "objectID": "pages/technical_writing/llm_agent/progress.html#tool-use-1",
    "href": "pages/technical_writing/llm_agent/progress.html#tool-use-1",
    "title": "LLM Agent",
    "section": "Tool Use",
    "text": "Tool Use\n사람은 신체/인지적 한계를 벗어나는 일을 수행하기 위해 외부 물체(external tool)를 만들고, 수정하고 활용합니다. LLM에 External Tool를 장착하면 모델의 기능이 크게 확장될 수 있습니다."
  }
]